{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab475d7",
   "metadata": {},
   "source": [
    "# Basic Sequence Models → RNNs (**Docs+Comments Edition**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e212ae7",
   "metadata": {},
   "source": [
    "## 0) 준비\n",
    "PyTorch가 없다면 자동 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "32f868a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    !pip -q install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "import torch, random, re\n",
    "from collections import defaultdict\n",
    "print('PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8652d89",
   "metadata": {},
   "source": [
    "## 1) 텍스트 n-gram 모델 (bi/tri/4-gram)\n",
    "작은 문장 코퍼스로 n-gram 카운트를 만들고 다음 단어를 샘플링합니다.\n",
    "\n",
    "핵심 포인트:\n",
    "- `build_ngram_counts`: n-gram과 context 카운트 계산\n",
    "- `next_probs`: 라플라스 스무딩 포함 다음 단어 확률 분포\n",
    "- `generate_sentence`: `<s>`에서 출발해 `</s>`가 나오면 종료\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df858a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_corpus 입력 문장 목록:\n",
      "  1: the students opened their books\n",
      "  2: the students opened their minds\n",
      "  3: the teacher entered the classroom\n",
      "  4: students love to solve problems\n",
      "  5: the classroom is full of books\n",
      "  6: open your mind and read more books\n",
      "  7: solve more problems to learn more\n",
      "build_corpus 입력 문장 수: 7\n"
     ]
    }
   ],
   "source": [
    "# 작은 예시 코퍼스 정의\n",
    "raw_sentences = [\n",
    "    'the students opened their books',\n",
    "    'the students opened their minds',\n",
    "    'the teacher entered the classroom',\n",
    "    'students love to solve problems',\n",
    "    'the classroom is full of books',\n",
    "    'open your mind and read more books',\n",
    "    'solve more problems to learn more',\n",
    "]\n",
    "\n",
    "print('build_corpus 입력 문장 목록:')\n",
    "for i, text in enumerate(raw_sentences, 1):\n",
    "    print(f'  {i}: {text}')\n",
    "print('build_corpus 입력 문장 수:', len(raw_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5ea5312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize 입력 문장: Open your Mind, and Read more books!!\n",
      "tokenize 출력 토큰: ['open', 'your', 'mind', 'and', 'read', 'more', 'books']\n"
     ]
    }
   ],
   "source": [
    "# 1. tokenize 함수와 예시\n",
    "def tokenize(s: str):\n",
    "    \"\"\"간단 토크나이저: 소문자화 후, 알파벳/숫자/공백만 남기고 분할.\"\"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-z0-9\\s]', '', s)\n",
    "    return s.split()\n",
    "\n",
    "example_sentence = 'Open your Mind, and Read more books!!'\n",
    "print('tokenize 입력 문장:', example_sentence)\n",
    "token_output = tokenize(example_sentence)\n",
    "print('tokenize 출력 토큰:', token_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f150a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_corpus 입력 문장 수: 7\n",
      "build_corpus 출력 토큰 시퀀스 앞 15개: ['<s>', 'the', 'students', 'opened', 'their', 'books', '</s>', '<s>', 'the', 'students', 'opened', 'their', 'minds', '</s>', '<s>']\n"
     ]
    }
   ],
   "source": [
    "# 2. build_corpus 함수와 예시\n",
    "START, END = '<s>', '</s>'\n",
    "\n",
    "def build_corpus(sentences):\n",
    "    \"\"\"문장 리스트를 `<s>`, `</s>` 포함 단일 토큰 리스트로 변환.\"\"\"\n",
    "    tokens = []\n",
    "    for s in sentences:\n",
    "        toks = tokenize(s)\n",
    "        tokens.extend([START] + toks + [END])\n",
    "    return tokens\n",
    "\n",
    "corpus = build_corpus(raw_sentences)\n",
    "print('build_corpus 입력 문장 수:', len(raw_sentences))\n",
    "print('build_corpus 출력 토큰 시퀀스 앞 15개:', corpus[:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa96fcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_ngram_counts 입력 n: 3\n",
      "build_ngram_counts 입력 토큰 수: 53\n",
      "build_ngram_counts 입력 컨텍스트: ('<s>', 'the')\n",
      "build_ngram_counts 출력 카운트: {('<s>', 'the', 'students'): 2, ('<s>', 'the', 'teacher'): 1, ('<s>', 'the', 'classroom'): 1}\n"
     ]
    }
   ],
   "source": [
    "# 3. build_ngram_counts 함수와 예시\n",
    "def build_ngram_counts(tokens, n: int = 3):\n",
    "    \"\"\"n-gram과 context 빈도수를 계산.\"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    ctx_counts = defaultdict(int)\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        ctx = ngram[:-1]\n",
    "        counts[ngram] += 1\n",
    "        ctx_counts[ctx] += 1\n",
    "    return counts, ctx_counts\n",
    "\n",
    "tri_counts, tri_ctx_counts = build_ngram_counts(corpus, n=3)\n",
    "sample_ctx = ('<s>', 'the')\n",
    "context_counts = {k: v for k, v in tri_counts.items() if k[:-1] == sample_ctx}\n",
    "print('build_ngram_counts 입력 n:', 3)\n",
    "print('build_ngram_counts 입력 토큰 수:', len(corpus))\n",
    "print('build_ngram_counts 입력 컨텍스트:', sample_ctx)\n",
    "print('build_ngram_counts 출력 카운트:', context_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fec1a73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_probs 입력 컨텍스트: ('<s>', 'the')\n",
      "next_probs 입력 delta: 1.0\n",
      "next_probs 출력 확률 상위 5개: [('students', 0.10344827586206899), ('classroom', 0.06896551724137934), ('teacher', 0.06896551724137934), ('</s>', 0.03448275862068967), ('<s>', 0.03448275862068967)]\n"
     ]
    }
   ],
   "source": [
    "# 4. next_probs 함수와 예시\n",
    "def next_probs(context, counts, ctx_counts, vocab, delta: float = 1.0):\n",
    "    \"\"\"라플라스 스무딩으로 `P(next|context)` 계산.\"\"\"\n",
    "    ctx = tuple(context)\n",
    "    total = ctx_counts.get(ctx, 0)\n",
    "    V = len(vocab)\n",
    "    probs = {}\n",
    "    for w in vocab:\n",
    "        c = counts.get(ctx + (w,), 0)\n",
    "        probs[w] = (c + delta) / (total + delta * V) if (total + delta * V) > 0 else 1.0 / V\n",
    "    s = sum(probs.values())\n",
    "    for k in probs:\n",
    "        probs[k] /= s\n",
    "    return probs\n",
    "\n",
    "vocab = sorted(set(corpus))\n",
    "probs_example = next_probs(sample_ctx, tri_counts, tri_ctx_counts, vocab, delta=1.0)\n",
    "top5 = sorted(probs_example.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print('next_probs 입력 컨텍스트:', sample_ctx)\n",
    "print('next_probs 입력 delta:', 1.0)\n",
    "print('next_probs 출력 확률 상위 5개:', top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1133c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_dict 입력 확률 분포: {'A': 0.1, 'B': 0.2, 'C': 0.7}\n",
      "sample_dict 출력 샘플 5회: ['C', 'C', 'B', 'C', 'C']\n"
     ]
    }
   ],
   "source": [
    "# 5. sample_dict 함수와 예시\n",
    "def sample_dict(probs: dict):\n",
    "    \"\"\"확률 분포에서 1개 토큰 샘플링.\"\"\"\n",
    "    items, weights = list(probs.keys()), list(probs.values())\n",
    "    return random.choices(items, weights=weights, k=1)[0]\n",
    "\n",
    "toy_probs = {'A': 0.1, 'B': 0.2, 'C': 0.7}\n",
    "print('sample_dict 입력 확률 분포:', toy_probs)\n",
    "samples = [sample_dict(toy_probs) for _ in range(5)]\n",
    "print('sample_dict 출력 샘플 5회:', samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b9fa1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_sentence 목적: n-gram LM이 시작 컨텍스트에서 다음 단어를 반복 샘플링하는 과정을 보여줍니다.\n",
      "generate_sentence 입력 값: n=2, delta=1.0, max_len=15\n",
      "  기본 초기 컨텍스트: ['<s>'] (n-1개의 `<s>` 토큰)\n",
      "  첫 문장 기반 초기 컨텍스트: ['<s>']\n",
      "  출력 샘플 (기본 컨텍스트): books\n",
      "  출력 샘플 (첫 문장 컨텍스트): love minds teacher classroom\n",
      "generate_sentence 입력 값: n=3, delta=1.0, max_len=15\n",
      "  기본 초기 컨텍스트: ['<s>', '<s>'] (n-1개의 `<s>` 토큰)\n",
      "  첫 문장 기반 초기 컨텍스트: ['<s>', 'the']\n",
      "  출력 샘플 (기본 컨텍스트): full solve teacher minds minds\n",
      "  출력 샘플 (첫 문장 컨텍스트): classroom their to teacher read of the the their more full students read\n",
      "generate_sentence 입력 값: n=4, delta=1.0, max_len=15\n",
      "  기본 초기 컨텍스트: ['<s>', '<s>', '<s>'] (n-1개의 `<s>` 토큰)\n",
      "  첫 문장 기반 초기 컨텍스트: ['<s>', 'the', 'students']\n",
      "  출력 샘플 (기본 컨텍스트): minds books and books books full their opened more their entered minds students solve\n",
      "  출력 샘플 (첫 문장 컨텍스트): to open learn their open teacher mind\n"
     ]
    }
   ],
   "source": [
    "# 6. generate_sentence 함수와 예시\n",
    "def generate_sentence(n: int = 3, delta: float = 1.0, max_len: int = 20, seed=None):\n",
    "    \"\"\"n-gram LM으로 문장 생성. `<s>`*(n-1)에서 시작, `</s>`에서 종료.\"\"\"\n",
    "    counts, ctx_counts = build_ngram_counts(corpus, n)\n",
    "    vocab = sorted(set(corpus))\n",
    "\n",
    "    # 초기 컨텍스트 설정\n",
    "    if seed is None:\n",
    "        ctx = [START] * (n - 1)\n",
    "    else:\n",
    "        ctx = seed\n",
    "\n",
    "    out = []\n",
    "    for _ in range(max_len):\n",
    "        probs = next_probs(ctx, counts, ctx_counts, vocab, delta)\n",
    "        w = sample_dict(probs)\n",
    "        if w == END:\n",
    "            break\n",
    "        if w not in (START, END):\n",
    "            out.append(w)\n",
    "        ctx = (ctx + [w])[1:]\n",
    "    return ' '.join(out)\n",
    "\n",
    "\n",
    "# ===== 예시 실행 =====\n",
    "print(\"generate_sentence 목적: n-gram LM이 시작 컨텍스트에서 다음 단어를 반복 샘플링하는 과정을 보여줍니다.\")\n",
    "\n",
    "first_sentence_tokens = tokenize(raw_sentences[0])\n",
    "\n",
    "for n in [2, 3, 4]:\n",
    "    # 1) 기본 초기 컨텍스트\n",
    "    default_seed = [START] * (n - 1)\n",
    "\n",
    "    # 2) 첫 문장 기반 초기 컨텍스트\n",
    "    example_seed = ([START] + first_sentence_tokens[:max(0, n - 2)])[:n - 1]\n",
    "\n",
    "    print(f\"generate_sentence 입력 값: n={n}, delta=1.0, max_len=15\")\n",
    "    print(\"  기본 초기 컨텍스트:\", default_seed, \"(n-1개의 `<s>` 토큰)\")\n",
    "    print(\"  첫 문장 기반 초기 컨텍스트:\", example_seed)\n",
    "\n",
    "    sentence_default = generate_sentence(n=n, delta=1.0, max_len=15, seed=default_seed)\n",
    "    sentence_seeded = generate_sentence(n=n, delta=1.0, max_len=15, seed=example_seed)\n",
    "\n",
    "    print(\"  출력 샘플 (기본 컨텍스트):\", sentence_default)\n",
    "    print(\"  출력 샘플 (첫 문장 컨텍스트):\", sentence_seeded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b35ca5c",
   "metadata": {},
   "source": [
    "## 2) 간단 RNN 문자 모델\n",
    "임베딩 -> RNN -> Linear 구조로 다음 문자를 예측합니다.\n",
    "\n",
    "- 입력: 선택한 문자 말뭉치를 정수 인덱스로 변환한 시퀀스(`data`).\n",
    "- 학습 목표: t 시점 문자를 입력하면 t+1 문자를 맞추도록 RNN을 학습시킵니다.\n",
    "- 출력: 학습된 RNN이 각 시점의 다음 문자 확률과 샘플링 텍스트를 제공합니다.\n",
    "\n",
    "아래 셀에서 데이터 소스를 결정한 뒤 CharRNN을 학습하고, 임의의 시작 문자와 길이로 문자를 생성해 봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "36dfaa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 데이터 입력 경로: data/simple_char_corpus.txt\n",
      "문자 데이터 총 길이: 339\n",
      "문자 데이터 앞 120자: The RNN workshop explores basic sequence models. / We start from n-gram language models and gradually move to recurrent id\n"
     ]
    }
   ],
   "source": [
    "# (옵션) 외부 파일에서 문자 코퍼스 불러오기\n",
    "from pathlib import Path\n",
    "corpus_path = Path('data/simple_char_corpus.txt')\n",
    "file_text = None\n",
    "if corpus_path.exists():\n",
    "    file_text = corpus_path.read_text()\n",
    "    preview = file_text[:120].replace('\\n', ' / ')\n",
    "    print('문자 데이터 입력 경로:', corpus_path)\n",
    "    print('문자 데이터 총 길이:', len(file_text))\n",
    "    print('문자 데이터 앞 120자:', preview)\n",
    "else:\n",
    "    print('data/simple_char_corpus.txt가 없어서 기본 샘플을 사용합니다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "452f493b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 소스: 외부 파일\n",
      "입력 문자 길이: 339\n",
      "고유 문자 수 (출력 차원): 35\n",
      "정수 시퀀스 길이: 339\n"
     ]
    }
   ],
   "source": [
    "# 문자 데이터 준비\n",
    "if file_text:\n",
    "    text = file_text\n",
    "    print('데이터 소스: 외부 파일')\n",
    "    print('입력 문자 길이:', len(text))\n",
    "else:\n",
    "    text_samples = [\n",
    "        'hello world hello pytorch',\n",
    "        'pytorch makes building rnns easier',\n",
    "        'sequence models learn next characters',\n",
    "        'hello sequence modeling with pytorch',\n",
    "        'rnn and gru handle sequential data',\n",
    "    ]\n",
    "    print('데이터 소스: 기본 샘플 리스트')\n",
    "    for i, sentence in enumerate(text_samples, 1):\n",
    "        print(f'  {i}: {sentence}')\n",
    "    text = '\\n'.join(text_samples) + '\\n'\n",
    "    print('결합된 입력 문자 길이:', len(text))\n",
    "\n",
    "chars = sorted(set(text))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
    "print('고유 문자 수 (출력 차원):', len(chars))\n",
    "print('정수 시퀀스 길이:', data.numel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "632b3b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN forward 입력 텐서 shape: torch.Size([1, 10])\n",
      "CharRNN forward 출력 로짓 shape: torch.Size([1, 10, 35])\n"
     ]
    }
   ],
   "source": [
    "# 1. CharRNN 클래스와 예시\n",
    "import torch.nn as nn\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    \"\"\"문자 단위 RNN 언어 모델.\"\"\"\n",
    "    def __init__(self, vocab_size: int, hidden_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        \"\"\"(B, T) 입력을 (B, T, V) 로짓으로 변환.\"\"\"\n",
    "        x = self.embed(x)\n",
    "        out, h = self.rnn(x, h)\n",
    "        return self.fc(out), h\n",
    "\n",
    "char_model = CharRNN(len(chars))\n",
    "sample_input = data[:10].unsqueeze(0)  # (1, 10)\n",
    "print('CharRNN forward 입력 텐서 shape:', sample_input.shape)\n",
    "logits, hidden = char_model(sample_input)\n",
    "print('CharRNN forward 출력 로짓 shape:', logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "97dcacf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_char_rnn 입력 값: seq_len=16, epochs=40, lr=1e-2\n",
      "epoch 0 loss 760.931\n",
      "epoch 10 loss 551.737\n",
      "epoch 20 loss 454.281\n",
      "epoch 30 loss 444.853\n",
      "epoch 40 loss 399.881\n",
      "epoch 50 loss 428.366\n",
      "epoch 60 loss 455.515\n",
      "epoch 70 loss 416.916\n",
      "epoch 80 loss 383.602\n",
      "epoch 90 loss 424.760\n"
     ]
    }
   ],
   "source": [
    "# 2. train_char_rnn 함수와 예시\n",
    "def train_char_rnn(model, data, seq_len=16, epochs=100, lr=1e-2):\n",
    "    \"\"\"한 글자 시프트된 타깃으로 교차엔트로피 학습.\"\"\"\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    V = model.fc.out_features\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for i in range(0, len(data) - seq_len - 1):\n",
    "            x = data[i:i+seq_len].unsqueeze(0)\n",
    "            y = data[i+1:i+seq_len+1].unsqueeze(0)\n",
    "            logits, _ = model(x)\n",
    "            loss = crit(logits.reshape(-1, V), y.reshape(-1))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        if ep % 10 == 0:\n",
    "            print(f'epoch {ep} loss {total:.3f}')\n",
    "\n",
    "print('train_char_rnn 입력 값: seq_len=16, epochs=40, lr=1e-2')\n",
    "train_char_rnn(char_model, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ef4022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_text 입력 값: start='h', length=80\n",
      "generate_text 출력 샘플:\n",
      "he modencies bal the modeng nthe modenting works bal the modencies buileveal leve\n"
     ]
    }
   ],
   "source": [
    "# 3. generate_text 함수와 예시\n",
    "def generate_text(model, start='h', length=80):\n",
    "    \"\"\"시작 문자와 길이를 받아 문자 시퀀스를 생성.\"\"\"\n",
    "    if start not in stoi:\n",
    "        raise ValueError('start \"{}\" is not in the vocabulary'.format(start))\n",
    "    model.eval()\n",
    "    x = torch.tensor([[stoi[start]]])\n",
    "    h = None\n",
    "    out = [start]\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            logits, h = model(x, h)\n",
    "            probs = torch.softmax(logits[0, -1], dim=0)\n",
    "            idx = torch.multinomial(probs, 1).item()\n",
    "        out.append(itos[idx])\n",
    "        x = torch.tensor([[idx]])\n",
    "    return ''.join(out)\n",
    "\n",
    "sample_start = 'h'\n",
    "sample_length = 80\n",
    "print(f\"generate_text 입력 값: start='{sample_start}', length={sample_length}\")\n",
    "generated = generate_text(char_model, sample_start, sample_length)\n",
    "print('generate_text 출력 샘플:')\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2742cfc",
   "metadata": {},
   "source": [
    "\n",
    "## 3) RNN 긴 문맥 한계 체험\n",
    "`[FLAG, noise..., 9, FLAG]` 패턴 데이터로 **기억 유지 능력**을 실험합니다.\n",
    "\n",
    "- **문제 정의**: 시퀀스 첫 토큰(FLAG)과 마지막 토큰(같은 FLAG) 사이에 긴 잡음 구간과 구분자(DELIM=9)가 끼어 있습니다. 모델이 끝에서 처음 FLAG를 맞혀야 합니다.\n",
    "- **용어 정리**\n",
    "  - `FLAG`: 시작/종료에서 짝을 이루는 신호 값(여기서는 1 또는 2).\n",
    "  - `DELIM`: 잡음 구간과 마지막 FLAG 사이를 구분하는 구분자 토큰(여기서는 9).\n",
    "  - `noise`: FLAG·DELIM이 아닌 0~8 사이 랜덤 숫자 토큰들이며, 기억을 방해하는 역할을 합니다.\n",
    "- **왜 어렵나?** 짧은 잡음 구간에서는 FLAG 정보를 유지하기 쉽지만, 구간이 길어질수록 잦은 비관련 토큰을 거쳐야 하므로 은닉 상태가 쉽게 잊혀집니다.\n",
    "- **실험 절차**\n",
    "  1. 데이터 생성 (FLAG/잡음/구분자 설명 포함)\n",
    "  2. 미니배치 구성 및 패딩 방식 확인\n",
    "  3. 단순 RNN 학습 (짧은 구간만 사용)\n",
    "  4. 다양한 잡음 길이에 대해 정확도 비교 (짧은 vs 긴)\n",
    "  5. 대표 시퀀스를 직접 넣어 예측/정답을 살펴보기\n",
    "- **기대 관찰**: 짧은 구간에서는 높은 정확도를 보이지만, 훈련 때 보지 못한 긴 구간으로 가면 정확도가 눈에 띄게 하락합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aa7b4f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토이 데이터 토큰 집합 (0~9): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "FLAG 후보: [1, 2] → 시작/종료 신호 역할\n",
      "DELIM 토큰: 9 → 잡음과 마지막 FLAG 사이의 구분자\n",
      "noise 토큰: FLAG/DELIM을 제외한 0~8 사이 숫자를 무작위로 사용\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 숫자 토이 데이터 설정\n",
    "VOCAB = list(range(10))\n",
    "DELIM = 9\n",
    "FLAGS = [1, 2]\n",
    "\n",
    "print('토이 데이터 토큰 집합 (0~9):', VOCAB)\n",
    "print('FLAG 후보:', FLAGS, '→ 시작/종료 신호 역할')\n",
    "print('DELIM 토큰:', DELIM, '→ 잡음과 마지막 FLAG 사이의 구분자')\n",
    "print('noise 토큰: FLAG/DELIM을 제외한 0~8 사이 숫자를 무작위로 사용')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8ff362bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_sample 입력 min_noise=4, max_noise=12 (기본값)\n",
      "make_sample 출력 시퀀스 길이: 14\n",
      "make_sample 출력 시퀀스: [1, 7, 5, 7, 0, 5, 1, 6, 0, 3, 4, 1, 9, 1]\n",
      "  구조 = [시작 FLAG, noise(0~8 랜덤)…, DELIM(9), 마지막 FLAG(정답)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. make_sample 함수와 예시\n",
    "def make_sample(min_noise=4, max_noise=12):\n",
    "    \"\"\"[FLAG, noise..., 9, FLAG] 시퀀스 하나 생성.\"\"\"\n",
    "    flag = random.choice(FLAGS)\n",
    "    noise = [random.randint(0, 8) for _ in range(random.randint(min_noise, max_noise))]\n",
    "    # noise는 FLAG/DELIM이 아닌 0~8 범위 숫자로 구성됩니다.\n",
    "    return [flag] + noise + [DELIM, flag]\n",
    "\n",
    "sample_seq = make_sample()\n",
    "print('make_sample 입력 min_noise=4, max_noise=12 (기본값)')\n",
    "print('make_sample 출력 시퀀스 길이:', len(sample_seq))\n",
    "print('make_sample 출력 시퀀스:', sample_seq)\n",
    "print('  구조 = [시작 FLAG, noise(0~8 랜덤)…, DELIM(9), 마지막 FLAG(정답)]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1f341efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchify 입력: batch_size=4, noise 길이 범위=[3, 5]\n",
      "batchify 출력 x 텐서 모양: (4, 7)\n",
      "batchify 출력 y 텐서 모양: (4, 7)\n",
      "batchify 출력 각 시퀀스 길이: [7, 5, 5, 5]\n",
      "batchify 첫 번째 입력 시퀀스: [1, 6, 6, 5, 6, 4, 9]\n",
      "batchify 첫 번째 타깃 시퀀스: [6, 6, 5, 6, 4, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. batchify 함수와 예시\n",
    "def batchify(batch_size=32, min_noise=4, max_noise=12):\n",
    "    \"\"\"가변 길이 샘플들을 0 패딩으로 (x, y, lengths) 배치 구성.\"\"\"\n",
    "    batch = [make_sample(min_noise, max_noise) for _ in range(batch_size)]\n",
    "    maxlen = max(len(s) for s in batch)\n",
    "    x = torch.full((batch_size, maxlen - 1), 0).long()\n",
    "    y = torch.full((batch_size, maxlen - 1), 0).long()\n",
    "    lengths = []\n",
    "    for i, seq in enumerate(batch):\n",
    "        inp, tgt = seq[:-1], seq[1:]\n",
    "        seq_len = len(inp)\n",
    "        x[i, :seq_len] = torch.tensor(inp)\n",
    "        y[i, :seq_len] = torch.tensor(tgt)\n",
    "        lengths.append(seq_len)\n",
    "    return x, y, torch.tensor(lengths)\n",
    "\n",
    "bx, by, blen = batchify(batch_size=4, min_noise=3, max_noise=5)\n",
    "print('batchify 입력: batch_size=4, noise 길이 범위=[3, 5]')\n",
    "print('batchify 출력 x 텐서 모양:', tuple(bx.shape))\n",
    "print('batchify 출력 y 텐서 모양:', tuple(by.shape))\n",
    "print('batchify 출력 각 시퀀스 길이:', blen.tolist())\n",
    "print('batchify 첫 번째 입력 시퀀스:', bx[0, :blen[0]].tolist())\n",
    "print('batchify 첫 번째 타깃 시퀀스:', by[0, :blen[0]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11e37b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNNLM 입력 배치 모양: (4, 7)\n",
      "SimpleRNNLM 출력 로짓 모양: (4, 7, 10)\n",
      "SimpleRNNLM 출력 마지막 시퀀스 로짓 상위 5개: [0.0472, 0.1179, -0.2466, -0.429, -0.0744]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. SimpleRNNLM 클래스와 예시\n",
    "class SimpleRNNLM(nn.Module):\n",
    "    \"\"\"단순 RNN 언어모델.\"\"\"\n",
    "    def __init__(self, vocab_size=10, hidden=16):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden)\n",
    "        self.rnn = nn.RNN(hidden, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        x = self.embed(x)\n",
    "        out, h = self.rnn(x, h)\n",
    "        return self.fc(out), h\n",
    "\n",
    "seq_model = SimpleRNNLM()\n",
    "batch_logits, _ = seq_model(bx)\n",
    "print('SimpleRNNLM 입력 배치 모양:', tuple(bx.shape))\n",
    "print('SimpleRNNLM 출력 로짓 모양:', tuple(batch_logits.shape))\n",
    "print('SimpleRNNLM 출력 마지막 시퀀스 로짓 상위 5개:', [round(v, 4) for v in batch_logits[0, blen[0]-1, :5].detach().cpu().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f5054720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_short_context: noise_len 범위=(4, 12), epochs=5, steps/epoch=120\n",
      "  epoch 00 평균 loss 0.7724\n",
      "  epoch 01 평균 loss 0.7032\n",
      "  epoch 02 평균 loss 0.6988\n",
      "  epoch 03 평균 loss 0.7011\n",
      "  epoch 04 평균 loss 0.7002\n",
      "train_short_context 완료: 모델 가중치가 갱신되었습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. train_short_context 함수와 예시\n",
    "def train_short_context(model, epochs=6, steps_per_epoch=120, clip=1.0, noise_range=(4, 12)):\n",
    "    \"\"\"noise_range 범위(짧은 구간) 데이터로 학습.\"\"\"\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    model.train()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    noise_min, noise_max = noise_range\n",
    "    print(f'train_short_context: noise_len 범위={noise_range}, epochs={epochs}, steps/epoch={steps_per_epoch}')\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for _ in range(steps_per_epoch):\n",
    "            x, y, lengths = batchify(32, noise_min, noise_max)\n",
    "            logits, _ = model(x)\n",
    "            last_idx = lengths - 1\n",
    "            batch_ids = torch.arange(x.size(0))\n",
    "            final_logits = logits[batch_ids, last_idx]\n",
    "            final_targets = y[batch_ids, last_idx]\n",
    "            loss = crit(final_logits, final_targets)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        print(f'  epoch {ep:02d} 평균 loss {total/steps_per_epoch:.4f}')\n",
    "    print('train_short_context 완료: 모델 가중치가 갱신되었습니다.')\n",
    "\n",
    "train_short_context(seq_model, epochs=5, steps_per_epoch=120, noise_range=(4, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3dad6963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잡음 길이별 정확도 비교 (trials=400):\n",
      "  noise_len=  6: 정확도 0.505 ← 훈련 분포\n",
      "  noise_len= 12: 정확도 0.505 ← 훈련 분포\n",
      "  noise_len= 24: 정확도 0.500 ← 미학습 길이\n",
      "  noise_len= 48: 정확도 0.470 ← 미학습 길이\n",
      "  noise_len= 72: 정확도 0.542 ← 미학습 길이\n",
      "  noise_len= 96: 정확도 0.537 ← 미학습 길이\n",
      "  단기 평균(<=12): 0.505 / 장기 평균(>=48): 0.540\n",
      "  → 긴 구간으로 갈수록 정확도가 떨어지는지를 확인하세요.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. eval_on_length 함수와 길이별 비교\n",
    "def eval_on_length(model, noise_len=80, trials=200):\n",
    "    \"\"\"noise_len 길이로 평가: DELIM 직후 FLAG를 맞추는 정확도.\"\"\"\n",
    "    model.eval()\n",
    "    corr = 0\n",
    "    random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(trials):\n",
    "            seq = [random.choice(FLAGS)] + [random.randint(0, 8) for _ in range(noise_len)] + [DELIM]\n",
    "            x = torch.tensor(seq).unsqueeze(0)\n",
    "            logits, _ = model(x)\n",
    "            pred = logits[0, -1].argmax().item()\n",
    "            gold = seq[0]\n",
    "            corr += int(pred == gold)\n",
    "    return corr / trials\n",
    "\n",
    "probe_lengths = [6, 12, 24, 48, 72, 96]\n",
    "accuracies = [(L, eval_on_length(seq_model, L, trials=400)) for L in probe_lengths]\n",
    "\n",
    "print('잡음 길이별 정확도 비교 (trials=400):')\n",
    "for L, acc in accuracies:\n",
    "    marker = '← 훈련 분포' if L <= 12 else '← 미학습 길이'\n",
    "    print(f'  noise_len={L:3d}: 정확도 {acc:.3f} {marker}')\n",
    "short_avg = sum(acc for L, acc in accuracies if L <= 12) / len([L for L in probe_lengths if L <= 12])\n",
    "long_avg = sum(acc for L, acc in accuracies if L > 48) / len([L for L in probe_lengths if L > 48])\n",
    "print(f'  단기 평균(<=12): {short_avg:.3f} / 장기 평균(>=48): {long_avg:.3f}')\n",
    "print('  → 긴 구간으로 갈수록 정확도가 떨어지는지를 확인하세요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f5e93b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspect_sequence: 잡음 길이에 따른 개별 예측 비교\n",
      "단기 잡음 샘플 (noise_len=10)\n",
      "  noise_len= 10 | 입력 FLAG=1 | 예측 FLAG=2 | 확신도=0.589\n",
      "    시퀀스 앞부분: [1, 5, 7, 8, 1, 3, 8, 4] ... 끝부분: [1, 3, 8, 4, 6, 8, 7, 9]\n",
      "  noise_len= 10 | 입력 FLAG=1 | 예측 FLAG=2 | 확신도=0.588\n",
      "    시퀀스 앞부분: [1, 3, 6, 0, 4, 2, 6, 7] ... 끝부분: [4, 2, 6, 7, 7, 8, 3, 9]\n",
      "  noise_len= 10 | 입력 FLAG=1 | 예측 FLAG=2 | 확신도=0.589\n",
      "    시퀀스 앞부분: [1, 5, 2, 3, 2, 2, 4, 8] ... 끝부분: [2, 2, 4, 8, 7, 7, 7, 9]\n",
      "장기 잡음 샘플 (noise_len=80)\n",
      "  noise_len= 80 | 입력 FLAG=2 | 예측 FLAG=2 | 확신도=0.590\n",
      "    시퀀스 앞부분: [2, 1, 2, 2, 2, 1, 6, 7] ... 끝부분: [3, 8, 7, 6, 6, 8, 2, 9]\n",
      "  noise_len= 80 | 입력 FLAG=2 | 예측 FLAG=2 | 확신도=0.587\n",
      "    시퀀스 앞부분: [2, 5, 6, 7, 3, 0, 8, 2] ... 끝부분: [0, 1, 2, 3, 0, 8, 0, 9]\n",
      "  noise_len= 80 | 입력 FLAG=2 | 예측 FLAG=2 | 확신도=0.587\n",
      "    시퀀스 앞부분: [2, 1, 6, 4, 4, 3, 5, 3] ... 끝부분: [3, 1, 5, 5, 5, 7, 0, 9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. 잡음 길이별 시퀀스 예시 확인\n",
    "def inspect_sequence(model, noise_len, trials=1):\n",
    "    for t in range(trials):\n",
    "        seq = [random.choice(FLAGS)] + [random.randint(0, 8) for _ in range(noise_len)] + [DELIM]\n",
    "        x = torch.tensor(seq).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(x)\n",
    "            probs = torch.softmax(logits[0, -1], dim=0)\n",
    "            pred = probs.argmax().item()\n",
    "            confidence = probs[pred].item()\n",
    "        print(f'  noise_len={noise_len:3d} | 입력 FLAG={seq[0]} | 예측 FLAG={pred} | 확신도={confidence:.3f}')\n",
    "        print('    시퀀스 앞부분:', seq[:8], '... 끝부분:', seq[-8:])\n",
    "\n",
    "print('inspect_sequence: 잡음 길이에 따른 개별 예측 비교')\n",
    "print('단기 잡음 샘플 (noise_len=10)')\n",
    "inspect_sequence(seq_model, noise_len=10, trials=3)\n",
    "print('장기 잡음 샘플 (noise_len=80)')\n",
    "inspect_sequence(seq_model, noise_len=80, trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f50f23e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khyun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
